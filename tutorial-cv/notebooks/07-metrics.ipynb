{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/data')\n",
    "sys.path.append('../src/features')\n",
    "sys.path.append('../src/models')\n",
    "sys.path.append('../src/visualization')\n",
    "from utils import *\n",
    "from data_module import *\n",
    "from model import *\n",
    "from dataset import *\n",
    "from visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14,10)\n",
    "font = {'family' : 'DejaVu Sans',  'weight' : 'normal',  'size'  : 22}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/work/interim/dataset_strat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bboxes\"] = df[\"bboxes\"].apply(eval) # DONT forget this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm = BoltDataModule(df, num_workers = 16, bs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tf_efficientnetv2_l', 'backbone_name': 'tf_efficientnetv2_l', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [1024, 1024], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 4.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 3, 'fpn_cell_repeats': 3, 'fpn_channels': 88, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': None, 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': ''}\n",
      "backbone True\n",
      "fpn True\n",
      "class_net True\n",
      "box_net True\n",
      "108\n",
      "36\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "model = BoltDetector(df, bs=3, num_classes=1, img_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['epoch=14-valid_loss=0.44.ckpt', 'epoch=13-valid_loss=0.48.ckpt']\n",
      "../../bolt_logs/detector/epoch=14-valid_loss=0.44.ckpt\n"
     ]
    }
   ],
   "source": [
    "#path = '/media/scratch/astamoulakatos/NMIS/detector/detector-epoch=83-valid_loss=0.57.ckpt'\n",
    "# Get the latest model\n",
    "#path = '/media/scratch/astamoulakatos/NMIS/detector/detector-epoch=83-valid_loss=0.57.ckpt'\n",
    "import os\n",
    "detector=\"../../bolt_logs/detector\"\n",
    "for root, dirs, files in os.walk(detector, topdown=False):\n",
    "  print(files)\n",
    "  for name in files:\n",
    "    path= os.path.join(detector, name)\n",
    "    break\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tf_efficientnetv2_l', 'backbone_name': 'tf_efficientnetv2_l', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [1024, 1024], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 4.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 3, 'fpn_cell_repeats': 3, 'fpn_channels': 88, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': None, 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': ''}\n",
      "backbone True\n",
      "fpn True\n",
      "class_net True\n",
      "box_net True\n",
      "108\n",
      "36\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "model = model.load_from_checkpoint(path, map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = model.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/effdet-0.3.0-py3.10.egg/effdet/bench.py:55: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  indices_all = cls_topk_indices_all // num_classes\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:377: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n",
      "/opt/conda/lib/python3.10/site-packages/effdet-0.3.0-py3.10.egg/effdet/bench.py:55: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  indices_all = cls_topk_indices_all // num_classes\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for batch in loader:\n",
    "    output = model.validation_step(batch=batch, batch_idx=0)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.405\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.564\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.522\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.405\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.043\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.435\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': tensor(0.4437),\n",
       " 'metrics': {'AP_all': 0.40538368913941003,\n",
       "  'AP_all_IOU_0_50': 0.5637623762376237,\n",
       "  'AP_all_IOU_0_75': 0.5224696843834202,\n",
       "  'AP_small': -1.0,\n",
       "  'AP_medium': 0.40538368913941003,\n",
       "  'AP_large': -1.0,\n",
       "  'AR_all_dets_1': 0.004,\n",
       "  'AR_all_dets_10': 0.04342857142857143,\n",
       "  'AR_all': 0.43485714285714294,\n",
       "  'AR_small': -1.0,\n",
       "  'AR_medium': 0.43485714285714294,\n",
       "  'AR_large': -1.0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for batch in loader:\n",
    "    output = model.validation_step(batch=batch, batch_idx=0)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.26s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.384\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.467\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.044\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': tensor(0.3750),\n",
       " 'metrics': {'AP_all': 0.38351518893288955,\n",
       "  'AP_all_IOU_0_50': 0.5516518195937241,\n",
       "  'AP_all_IOU_0_75': 0.4668547640708516,\n",
       "  'AP_small': -1.0,\n",
       "  'AP_medium': 0.38351518893288955,\n",
       "  'AP_large': -1.0,\n",
       "  'AR_all_dets_1': 0.005389221556886228,\n",
       "  'AR_all_dets_10': 0.04431137724550898,\n",
       "  'AR_all': 0.42215568862275443,\n",
       "  'AR_small': -1.0,\n",
       "  'AR_medium': 0.42215568862275443,\n",
       "  'AR_large': -1.0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': tensor([[[8.8570e+02, 1.5380e+02, 9.3887e+02, 2.2195e+02, 7.3911e-01,\n",
       "           1.0000e+00],\n",
       "          [8.3846e+01, 3.6645e+02, 1.3663e+02, 4.3459e+02, 7.3621e-01,\n",
       "           1.0000e+00],\n",
       "          [8.8562e+02, 6.0749e+02, 9.3834e+02, 6.7525e+02, 7.2081e-01,\n",
       "           1.0000e+00],\n",
       "          ...,\n",
       "          [8.5859e+02, 8.3636e+02, 1.0240e+03, 9.4945e+02, 4.5974e-02,\n",
       "           1.0000e+00],\n",
       "          [8.2359e+01, 3.7756e+02, 1.0848e+02, 4.0890e+02, 4.5927e-02,\n",
       "           1.0000e+00],\n",
       "          [9.0192e+02, 1.7062e+02, 9.3013e+02, 2.1373e+02, 4.5920e-02,\n",
       "           1.0000e+00]],\n",
       " \n",
       "         [[3.6834e+02, 8.1228e+02, 4.2132e+02, 8.8004e+02, 7.6193e-01,\n",
       "           1.0000e+00],\n",
       "          [9.1263e+02, 1.0242e+02, 9.6568e+02, 1.7049e+02, 7.5736e-01,\n",
       "           1.0000e+00],\n",
       "          [9.2127e+01, 1.0725e+02, 1.4517e+02, 1.7613e+02, 7.5618e-01,\n",
       "           1.0000e+00],\n",
       "          ...,\n",
       "          [9.4755e+01, 7.7762e+02, 1.4478e+02, 8.5007e+02, 6.7075e-02,\n",
       "           1.0000e+00],\n",
       "          [9.8415e+01, 8.3341e+02, 1.2579e+02, 8.6560e+02, 6.7014e-02,\n",
       "           1.0000e+00],\n",
       "          [6.1966e+02, 7.7536e+02, 6.7187e+02, 8.4332e+02, 6.6868e-02,\n",
       "           1.0000e+00]]]),\n",
       " 'targets': [{'bboxes': tensor([[153.7354, 883.8095, 221.1217, 935.8730],\n",
       "           [606.4762, 886.3492, 672.5079, 937.1429],\n",
       "           [366.3915,  80.2540, 434.1164, 132.3175]]),\n",
       "   'labels': tensor([1, 1, 1]),\n",
       "   'bolts': 3,\n",
       "   'img_size': [1024, 1024],\n",
       "   'img_scale': tensor([1.])},\n",
       "  {'bboxes': tensor([[113.1005,  94.2222, 179.1323, 146.2857],\n",
       "           [344.7196,  96.7619, 410.4127, 147.5556],\n",
       "           [582.7725,  95.4921, 650.4974, 145.0159],\n",
       "           [821.1640,  94.2222, 888.5503, 143.7460],\n",
       "           [104.6349, 643.0476, 172.3598, 693.8412],\n",
       "           [108.0212, 910.4762, 174.0529, 962.5397],\n",
       "           [339.6402, 910.4762, 407.0265, 960.0000],\n",
       "           [572.6138, 911.7460, 640.3386, 962.5397],\n",
       "           [816.0847, 369.2698, 883.4709, 420.0635],\n",
       "           [809.3122, 640.5079, 876.6984, 691.3016],\n",
       "           [807.6190, 916.8254, 875.0053, 966.3492]]),\n",
       "   'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'bolts': 11,\n",
       "   'img_size': [1024, 1024],\n",
       "   'img_scale': tensor([1.])}],\n",
       " 'bolts_counted': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['batch_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
