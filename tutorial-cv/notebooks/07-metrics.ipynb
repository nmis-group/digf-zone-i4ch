{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-zb4gcsw0 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/data')\n",
    "sys.path.append('../src/features')\n",
    "sys.path.append('../src/models')\n",
    "sys.path.append('../src/visualization')\n",
    "from utils import *\n",
    "from data_module import *\n",
    "from model import *\n",
    "from dataset import *\n",
    "from visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14,10)\n",
    "font = {'family' : 'DejaVu Sans',  'weight' : 'normal',  'size'  : 22}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../work/interim/dataset_strat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bboxes\"] = df[\"bboxes\"].apply(eval) # DONT forget this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm = BoltDataModule(df, num_workers = 16, bs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tf_efficientnetv2_l', 'backbone_name': 'tf_efficientnetv2_l', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [1024, 1024], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 4.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 3, 'fpn_cell_repeats': 3, 'fpn_channels': 88, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': None, 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': ''}\n",
      "backbone True\n",
      "fpn True\n",
      "class_net True\n",
      "box_net True\n",
      "108\n",
      "36\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "model = BoltDetector(df, bs=3, num_classes=1, img_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/COREF/src/Bolt_Detection/detector/epoch=8-valid_loss=1.46.ckpt\n"
     ]
    }
   ],
   "source": [
    "#path = '/media/scratch/astamoulakatos/NMIS/detector/detector-epoch=83-valid_loss=0.57.ckpt'\n",
    "# Get the latest model\n",
    "#path = '/media/scratch/astamoulakatos/NMIS/detector/detector-epoch=83-valid_loss=0.57.ckpt'\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"./detector\", topdown=False):\n",
    "  for name in files:\n",
    "    path= os.path.join(os.getcwd(), \"detector\", name)\n",
    "    break\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tf_efficientnetv2_l', 'backbone_name': 'tf_efficientnetv2_l', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [1024, 1024], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 4.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 3, 'fpn_cell_repeats': 3, 'fpn_channels': 88, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': None, 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': ''}\n",
      "backbone True\n",
      "fpn True\n",
      "class_net True\n",
      "box_net True\n",
      "108\n",
      "36\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "model = model.load_from_checkpoint(path, map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = model.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/effdet/bench.py:55: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  indices_all = cls_topk_indices_all // num_classes\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py:374: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for batch in loader:\n",
    "    output = model.validation_step(batch=batch, batch_idx=0)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': tensor(1.4647),\n",
       " 'metrics': {'AP_all': -1,\n",
       "  'AP_all_IOU_0_50': -1,\n",
       "  'AP_all_IOU_0_75': -1,\n",
       "  'AP_small': -1,\n",
       "  'AP_medium': -1,\n",
       "  'AP_large': -1,\n",
       "  'AR_all_dets_1': -1,\n",
       "  'AR_all_dets_10': -1,\n",
       "  'AR_all': -1,\n",
       "  'AR_small': -1,\n",
       "  'AR_medium': -1,\n",
       "  'AR_large': -1}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for batch in loader:\n",
    "    output = model.validation_step(batch=batch, batch_idx=0)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.39s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.347\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.559\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.422\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.347\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': tensor(0.3298),\n",
       " 'metrics': {'AP_all': 0.34736314309606564,\n",
       "  'AP_all_IOU_0_50': 0.5592909613541999,\n",
       "  'AP_all_IOU_0_75': 0.4217663324844231,\n",
       "  'AP_small': 0.34736314309606564,\n",
       "  'AP_medium': -1.0,\n",
       "  'AP_large': -1.0,\n",
       "  'AR_all_dets_1': 0.002890173410404624,\n",
       "  'AR_all_dets_10': 0.03757225433526011,\n",
       "  'AR_all': 0.3901734104046243,\n",
       "  'AR_small': 0.3901734104046243,\n",
       "  'AR_medium': -1.0,\n",
       "  'AR_large': -1.0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': tensor([[[4.3359e+02, 3.1790e+02, 4.6926e+02, 3.4523e+02, 9.1606e-01,\n",
       "           1.0000e+00],\n",
       "          [1.9348e+02, 4.5812e+02, 2.2830e+02, 4.8432e+02, 8.8686e-01,\n",
       "           1.0000e+00],\n",
       "          [1.9573e+02, 3.7775e+01, 2.2913e+02, 6.3065e+01, 8.6961e-01,\n",
       "           1.0000e+00],\n",
       "          ...,\n",
       "          [7.7896e+01, 1.1460e+02, 1.0592e+02, 1.5244e+02, 2.3341e-02,\n",
       "           1.0000e+00],\n",
       "          [5.3021e+01, 1.8847e+02, 7.9952e+01, 2.2465e+02, 2.2966e-02,\n",
       "           1.0000e+00],\n",
       "          [1.8094e+02, 2.4354e+02, 3.5121e+02, 2.8162e+02, 2.2759e-02,\n",
       "           1.0000e+00]],\n",
       " \n",
       "         [[4.3150e+02, 2.4583e+02, 4.5714e+02, 2.7936e+02, 6.1033e-01,\n",
       "           1.0000e+00],\n",
       "          [1.5861e+02, 7.9295e+00, 1.8407e+02, 4.0756e+01, 5.4415e-01,\n",
       "           1.0000e+00],\n",
       "          [3.9826e+01, 2.3946e+02, 6.6026e+01, 2.7401e+02, 4.8633e-01,\n",
       "           1.0000e+00],\n",
       "          ...,\n",
       "          [1.9900e+02, 6.6271e+01, 2.2581e+02, 1.0245e+02, 4.2846e-02,\n",
       "           1.0000e+00],\n",
       "          [2.2099e+02, 7.3563e+01, 2.4743e+02, 1.1111e+02, 4.2223e-02,\n",
       "           1.0000e+00],\n",
       "          [4.6334e+01, 1.4168e+02, 7.2992e+01, 1.7784e+02, 4.2071e-02,\n",
       "           1.0000e+00]]]),\n",
       " 'targets': [{'bboxes': tensor([[317.7143, 436.8254, 344.6349, 471.7037],\n",
       "           [ 36.3175, 195.7249,  61.7143, 229.2487],\n",
       "           [455.3651, 193.3545, 479.7460, 225.8624]]),\n",
       "   'labels': tensor([1, 1, 1]),\n",
       "   'bolts': 3,\n",
       "   'img_size': [512, 512],\n",
       "   'img_scale': tensor([1.])},\n",
       "  {'bboxes': tensor([[  5.0794, 158.7302,  39.6190, 183.3651],\n",
       "           [ 86.1799, 119.3651, 119.8730, 144.1270],\n",
       "           [400.4233, 224.6349, 433.4391, 250.5397],\n",
       "           [321.8624, 133.9682, 356.4021, 158.7302],\n",
       "           [244.1481,  42.6667, 276.9947,  68.0635],\n",
       "           [ 86.1799, 251.1746, 119.8730, 275.9365],\n",
       "           [326.9418, 392.5079, 359.7884, 417.9048],\n",
       "           [164.7407,  80.1270, 198.4339, 105.3968],\n",
       "           [247.5344, 434.2857, 281.2275, 459.6825],\n",
       "           [165.5873, 341.8413, 198.4339, 367.1111]]),\n",
       "   'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   'bolts': 10,\n",
       "   'img_size': [512, 512],\n",
       "   'img_scale': tensor([1.])}],\n",
       " 'bolts_counted': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['batch_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
